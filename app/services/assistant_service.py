import os #í™˜ê²½ ë³€ìˆ˜(OpenAI API Key) ì ‘ê·¼ìš©
from typing import Any, Dict, Optional, List, Tuple

from dotenv import load_dotenv #.env íŒŒì¼ ë¡œë“œ
from langchain_openai import ChatOpenAI #OpenAI GPT ëª¨ë¸ì„ LangChain ì¸í„°í˜ì´ìŠ¤ë¡œ ì‚¬ìš©
from langchain_core.messages import SystemMessage, HumanMessage #LLM í”„ë¡¬í”„íŠ¸ ë©”ì‹œì§€ êµ¬ì¡° ì •ì˜
import os
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple

from langchain_openai import ChatOpenAI


from langchain_core.messages import SystemMessage, HumanMessage
from langchain_core.documents import Document
from textwrap import shorten


from app.utils import vectorstore_state  #ì„œë²„ ì „ì—­ FAISS ë²¡í„°ìŠ¤í† ì–´ ì ‘ê·¼ìš© (RAG í•µì‹¬)

# =========================================================
# LLM
load_dotenv()

def get_llm():
    key = os.getenv("OPENAI_API_KEY")
    if not key:
        raise ValueError("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    return ChatOpenAI( #í‚¤ ì—†ìœ¼ë©´ ì„œë²„ ì„¤ì • ì˜¤ë¥˜ë¡œ ì¦‰ì‹œ ì‹¤íŒ¨
        model="gpt-4o",
        temperature=0.3,
        openai_api_key=key,
    )


# =========================================================
# System Prompt
# =========================================================
def build_system_prompt(mode: str) -> str:
    if mode == "spec":
        return (
            "You are an expert in defining specifications for robotics and automation projects "
            "based on NVIDIA Isaac Sim and Isaac Lab. "
            "Based on the user's requirements, you logically structure and explain the components "
            "of a robot learning environment, such as Action, Observation, Reward, and Termination."
        )

    if mode == "params":
        return (
            "You are an expert in designing robot learning parameters in NVIDIA Isaac Sim and Isaac Lab environments. "
            "Your goal is to propose realistic parameter ranges that prioritize reinforcement learning stability "
            "and reliable convergence."
        )

    if mode == "template":
        return (
            "You are an assistant specialized in creating documentation and configuration templates "
            "for robot learning and automation projects. "
            "You generate structured outputs that users can directly copy and use."
        )

    return "You are a helpful assistant specialized in NVIDIA Isaac Sim / Isaac Lab robotics learning workflows."

# =========================================================
# ì¶œë ¥ í•¨ìˆ˜ (Top-k ë¶„ë¦¬ + score ì¶œë ¥)
# =========================================================
def print_retrieval(results: List[Tuple[Document, float]], max_preview_chars: int = 350):
    """
    results: List[Tuple[Document, float]] from similarity_search_with_score
    - scoreëŠ” FAISS ì„¤ì •ì— ë”°ë¼ 'ê±°ë¦¬' ë˜ëŠ” 'ìœ ì‚¬ë„ ì„±ê²©'ì¼ ìˆ˜ ìˆì–´ ë¼ë²¨ì„ scoreë¡œ ë‘ .
    """
    print("\n" + "=" * 60)
    print("ğŸ” [RETRIEVED CONTEXT - TOP RESULTS]")
    print("=" * 60)

    for rank, (doc, score) in enumerate(results, start=1):
        src = doc.metadata.get("source_file", "unknown")
        chunk = doc.metadata.get("chunk_id", "?")
        print(f"\n--- [#{rank}] score: {score:.6f} | {src} | chunk {chunk} ---")

        preview = doc.page_content.strip().replace("\n", " ")
        preview = shorten(preview, width=max_preview_chars, placeholder=" ...")
        print(preview)

    print("\n" + "=" * 60 + "\n")


def print_full_docs(results: List[Tuple[Document, float]]):
    """Top-k ë¬¸ì„œ chunk ì „ì²´ ë‚´ìš©ì„ rankë³„ë¡œ ë¶„ë¦¬ ì¶œë ¥"""
    print("\n" + "=" * 60)
    print("ğŸ“„ [FULL CHUNKS - TOP RESULTS]")
    print("=" * 60)

    for rank, (doc, score) in enumerate(results, start=1):
        src = doc.metadata.get("source_file", "unknown")
        chunk = doc.metadata.get("chunk_id", "?")
        print(f"\n========== [#{rank}] score: {score:.6f} | {src} | chunk {chunk} ==========")
        print(doc.page_content.rstrip())
        print("=" * 60)

    print("\n" + "=" * 60 + "\n")


def docs_to_context(docs: List[Document]) -> str:
    blocks = []
    for d in docs:
        src = d.metadata.get("source_file", "unknown")
        chunk = d.metadata.get("chunk_id", "?")
        blocks.append(f"[{src} | chunk {chunk}]\n{d.page_content}")
    return "\n\n".join(blocks)

def docs_to_sources(docs: List[Document]) -> List[str]:
    seen = set()
    out = []
    for d in docs:
        s = f"{d.metadata.get('source_file','unknown')} | chunk {d.metadata.get('chunk_id','?')}"
        if s not in seen:
            seen.add(s)
            out.append(s)
    return out

# =========================================================
# âœ… ë””ë²„ê·¸ ì¶œë ¥ (í”„ë¡¬í”„íŠ¸/ì»¨í…ìŠ¤íŠ¸)
# =========================================================
def print_prompt_debug(
    system_prompt: str,
    user_prompt: str,
    context: str,
    sources: List[str],
    max_context_chars: int = 0,
):
    """
    - system_prompt, user_prompt: LLMì— ì „ë‹¬ë˜ëŠ” ì›ë¬¸
    - context: RAG context (user_promptì—ë„ í¬í•¨ë˜ì–´ ìˆì§€ë§Œ ë”°ë¡œ ë¶„ë¦¬ ì¶œë ¥)
    - sources: ì–´ë–¤ chunkê°€ ë“¤ì–´ê°”ëŠ”ì§€ í™•ì¸ìš©
    - max_context_chars:
        0ì´ë©´ ì „ì²´ ì¶œë ¥,
        0ë³´ë‹¤ í¬ë©´ contextë¥¼ ê·¸ ê¸¸ì´ë¡œ ì˜ë¼ ë¯¸ë¦¬ë³´ê¸°ë¡œ ì¶œë ¥
    """
    print("\n" + "#" * 90)
    print("ğŸ§ª [DEBUG] PROMPT INPUTS TO LLM")
    print("#" * 90)

    print("\n[SOURCES]")
    print("-" * 90)
    if sources:
        for s in sources:
            print("-", s)
    else:
        print("(no sources)")

    print("\n[SYSTEM PROMPT]")
    print("-" * 90)
    print(system_prompt)

    print("\n[CONTEXT]")
    print("-" * 90)
    if context:
        if max_context_chars and len(context) > max_context_chars:
            print(context[:max_context_chars] + "\n... (truncated)")
        else:
            print(context)
    else:
        print("(empty context)")

    print("\n[USER PROMPT (FULL)]")
    print("-" * 90)
    print(user_prompt)

    print("\n" + "#" * 90 + "\n")



# =========================================================
# RAG ì‹¤í–‰
# =========================================================
async def run_assistant_query( 
    mode: str,
    query: str,
    robot: Dict[str, Any],
    user: Optional[Dict[str, Any]] = None,
    top_k: int = 3,
    print_full_chunks: bool = False,     # âœ… chunk ì „ì²´ ì¶œë ¥ ì˜µì…˜
    debug_prompt: bool = True,           # âœ… í”„ë¡¬í”„íŠ¸/ì»¨í…ìŠ¤íŠ¸ ë””ë²„ê·¸ ì¶œë ¥ ì˜µì…˜
    debug_context_max_chars: int = 0,    # âœ… 0ì´ë©´ context ì „ì²´ ì¶œë ¥, ì•„ë‹ˆë©´ ì˜ë¼ì„œ ì¶œë ¥
    ) -> Dict[str, Any]:

    llm = get_llm()
    system_prompt = build_system_prompt(mode)


    context = ""
    sources: List[str] = []
    results: List[Tuple[Document, float]] = []

    if vectorstore_state.VECTORSTORE is not None:
        vs = vectorstore_state.VECTORSTORE
        print("vs is None?", vs is None)
        print("vs type:", type(vs))

        # ì¸ë±ìŠ¤ ì°¨ì›
        print("faiss dim:", vs.index.d)

        # ì¿¼ë¦¬ ì„ë² ë”© ì°¨ì›
        q_emb = vs.embedding_function.embed_query(query)
        print("query emb dim:", len(q_emb))
        print("query sample:", q_emb[:5])
        results = vectorstore_state.VECTORSTORE.similarity_search_with_score(query, k=top_k)

        # âœ… Top-k ìš”ì•½ ì¶œë ¥ + í•„ìš” ì‹œ ì „ì²´ chunk ì¶œë ¥
        if results:
            print_retrieval(results, max_preview_chars=350)
            if print_full_chunks:
                print_full_docs(results)

        docs = [doc for (doc, _score) in results]
        if docs:
            context = docs_to_context(docs)
            sources = docs_to_sources(docs)
    
    if context:
        user_prompt = f"""
Based on the following context, answer the question in detail.

Instructions:
- Actively use the context if it is directly relevant to the question.
- If the context does not contain sufficient information, you may also rely on general knowledge to answer.
- When combining context-based information with general knowledge, do not exaggerate.
- If any part of the answer is uncertain or inferred, clearly indicate it as "estimated" or "inferred".

[Context]
{context}

[User Question]
{query}

[Robot Info]
{robot}
"""
    else:
        user_prompt = f"""
Answer the following question.
(Since there are no reference documents provided, respond based on general knowledge and reasoning. If any part of the answer is uncertain, clearly indicate it as "estimated".)

[User Question]
{query}

[Robot Info]
{robot}
"""

    # âœ… ë””ë²„ê·¸: LLMì— ë“¤ì–´ê°€ëŠ” í”„ë¡¬í”„íŠ¸/ì»¨í…ìŠ¤íŠ¸ ì¶œë ¥
    if debug_prompt:
        print_prompt_debug(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            context=context,
            sources=sources,
            max_context_chars=debug_context_max_chars,
        )

    # âœ… ì‹¤ì œ invoke (system/user ë©”ì‹œì§€ ë³€ìˆ˜ë¡œ ë¶„ë¦¬)
    system_msg = SystemMessage(content=system_prompt)
    human_msg = HumanMessage(content=user_prompt)

    answer = llm.invoke([system_msg, human_msg]).content
    print("\nğŸ§  ANSWER:\n", answer)
    print("\nğŸ“š SOURCES:")
    for s in sources:
        print("-", s)

    raw_retrieval=[
            {
                "rank": i + 1,
                "score": float(score),
                "source_file": doc.metadata.get("source_file", "unknown"),
                "chunk_id": doc.metadata.get("chunk_id", "?"),
            }
            for i, (doc, score) in enumerate(results)
        ]
    print("\nğŸ“Œ Retrieval summary:")
    for r in raw_retrieval:
        print(f"- #{r['rank']} score={r['score']:.6f} | {r['source_file']} | chunk {r['chunk_id']}")

    return {
        "answer": answer,
        "sources": sources,
    }







# =========================================================
# ì‹¤í–‰
# =========================================================
# if __name__ == "__main__":
#     result = execute_rag_query(
#         mode="params",
#         query="I'm setting up ANYmal quadruped robot for rough terrain locomotion in Isaac Gym.What domain randomization parameters should I use for friction coefficient, mass distribution, and motor strength? What's the typical reward weight for velocity tracking vs energy consumption?",
#         robot={
#             "type": "quadruped",
#             "dof": 12,
#             "notes": "IsaacLab PPO training",
#         },
#         top_k=3,
#         print_full_chunks=True,         # Trueë©´ ê° chunk ì „ì²´ë¥¼ ë¶„ë¦¬ ì¶œë ¥
#         debug_prompt=True,             # âœ… Trueë©´ í”„ë¡¬í”„íŠ¸/ì»¨í…ìŠ¤íŠ¸ ë””ë²„ê·¸ ì¶œë ¥
#         debug_context_max_chars=0,      # âœ… 0ì´ë©´ context ì „ì²´ ì¶œë ¥ (ë„ˆë¬´ ê¸¸ë©´ ì ë‹¹íˆ ìˆ«ì ì§€ì •)
#     )

    
