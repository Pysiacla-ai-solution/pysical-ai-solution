# job_intern.py ìƒì„±
import time
import os
import sys

def log(msg):
    print(f"[Intern/Batch] {msg}", flush=True)

log(f"ğŸ’¤ ë°°ì¹˜ ì‘ì—… ì‹œì‘ (PID: {os.getpid()})")

try:
    import torch
    if torch.cuda.is_available():
        # 1GB VRAM í• ë‹¹ (float32ëŠ” 4ë°”ì´íŠ¸, ì•½ 2.5ì–µê°œ ìš”ì†Œ)
        # 250,000,000 * 4 bytes â‰ˆ 1GB
        tensor_size = 250_000_000
        data = torch.empty(tensor_size, dtype=torch.float32, device='cuda')
        log(f"âœ… VRAM 1GB í• ë‹¹ ì™„ë£Œ: {torch.cuda.get_device_name(0)}")
    else:
        log("âš ï¸ CUDA ì—†ìŒ: CPU ëª¨ë“œë¡œ 1GB í‰ë‚´ë§Œ ëƒ…ë‹ˆë‹¤.")
except ImportError:
    log("âš ï¸ PyTorch ì—†ìŒ: ë©”ëª¨ë¦¬ í• ë‹¹ ì—†ì´ ì§„í–‰í•©ë‹ˆë‹¤.")

# 60ì´ˆ ë™ì•ˆ ì²œì²œíˆ ì‹¤í–‰ (ë‹¤ë¥¸ ì‘ì—…ì´ ëŒ€ê¸°í•˜ê²Œ ë§Œë“¦)
total_time = 60
for i in range(total_time):
    log(f"ì—´ì‹¬íˆ ì¼í•˜ëŠ” ì¤‘... {i+1}/{total_time}ì´ˆ")
    time.sleep(1)

log("ğŸ‰ ë°°ì¹˜ ì‘ì—… ì™„ë£Œ!")
