
from __future__ import annotations

import asyncio
import time
import uuid
import math
from dataclasses import dataclass, field
from pathlib import Path
from typing import Dict, List, Optional, Literal

from .dirs import DirLayout
from .gpu_monitor import GpuMonitor
from .process_runner import ProcessRunner


@dataclass
class SlurmConfig:
    """
    SLURM ìŠ¤íƒ€ì¼ ìš°ì„ ìˆœìœ„ ê°€ì¤‘ì¹˜ ì„¤ì •.
    ê° í•­ëª©ì€ 0~1.0 ì‚¬ì´ì˜ Factorì™€ ê³±í•´ì ¸ ìµœì¢… ì ìˆ˜ê°€ ë©ë‹ˆë‹¤.
    """
    # Weights (ê°€ì¤‘ì¹˜)
    weight_age: float = 1000.0       # ëŒ€ê¸° ì‹œê°„ì´ ê¸¸ìˆ˜ë¡ ì ìˆ˜
    weight_fairshare: float = 10000.0 # ê³µì •ì„± (ì‚¬ìš©ëŸ‰ì´ ì ì„ìˆ˜ë¡ ì ìˆ˜) - ê°€ìž¥ í¼
    weight_job_size: float = 500.0   # ìž‘ì—… í¬ê¸° (í´ìˆ˜ë¡ ì ìˆ˜ or ìž‘ì„ìˆ˜ë¡ ì ìˆ˜)
    weight_partition: float = 1000.0 # íŒŒí‹°ì…˜ë³„ ê¸°ë³¸ ì ìˆ˜
    weight_qos: float = 1000.0       # QOS ë“±ê¸‰ë³„ ì ìˆ˜

    # Fair-share ì„¤ì •
    # ë°˜ê°ê¸° ë“± ë³µìž¡í•œ ë¡œì§ ëŒ€ì‹ , 'í‰ê·  ì‚¬ìš©ëŸ‰' ëŒ€ë¹„ ë‚´ ì‚¬ìš©ëŸ‰ì´ 2ë°°ë©´ ì ìˆ˜ 0.5ë°° ë˜ëŠ” ì‹ì˜ ê°ì‡  ê³„ìˆ˜
    fairshare_decay_norm: float = 3600.0 * 10  # 10ì‹œê°„ ì‚¬ìš©ì„ ê¸°ì¤€ìœ¼ë¡œ ì •ê·œí™”

    # Partition / QOS ì •ì˜ (ì´ë¦„ -> ì ìˆ˜ 0.0~1.0 Factor)
    partitions: Dict[str, float] = field(default_factory=lambda: {
        "debug": 1.0,    # ë†’ì€ ìš°ì„ ìˆœìœ„
        "normal": 0.5,   # ê¸°ë³¸
        "batch": 0.1     # ë°±ê·¸ë¼ìš´ë“œ
    })
    
    qos_levels: Dict[str, float] = field(default_factory=lambda: {
        "admin": 1.0,
        "premium": 0.8,
        "standard": 0.5,
        "guest": 0.1
    })


@dataclass
class Job:
    id: str
    script_path: Path
    user_id: str
    vram_required: int
    created_at: float
    
    # SLURM Factor ê´€ë ¨ í•„ë“œ ì¶”ê°€
    partition: str = "normal"
    qos: str = "standard"
    
    status: str = "QUEUED"
    assigned_gpu: Optional[int] = None
    pid: Optional[int] = None
    
    # ê³„ì‚°ëœ ìš°ì„ ìˆœìœ„ ì ìˆ˜ ë° ë””ë²„ê¹…ìš© íŒ©í„°
    priority_score: float = 0.0
    _debug_factors: Dict[str, float] = field(default_factory=dict)

    @property
    def time_waiting(self) -> float:
        return time.time() - self.created_at


class UserUsageTracker:
    """
    Fair-share ê³„ì‚°ì„ ìœ„í•œ ì‚¬ìš©ìžë³„ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ì¶”ì ê¸°.
    (ë©”ëª¨ë¦¬ ë‚´ ì €ìž¥, ì‹¤ì œ êµ¬í˜„ ì‹œ DB í•„ìš”)
    """
    def __init__(self) -> None:
        # user_id -> total_gpu_seconds
        self._usage: Dict[str, float] = {}
        self._lock = asyncio.Lock()

    async def add_usage(self, user_id: str, duration_sec: float, gpu_count: int = 1) -> None:
        async with self._lock:
            current = self._usage.get(user_id, 0.0)
            # GPU ì‚¬ìš© ì‹œê°„ = ì‹œê°„ * GPU ê°œìˆ˜
            self._usage[user_id] = current + (duration_sec * gpu_count)

    async def get_usage(self, user_id: str) -> float:
        async with self._lock:
            return self._usage.get(user_id, 0.0)

    async def get_total_usage(self) -> float:
        async with self._lock:
            return sum(self._usage.values())


class InMemoryJobStore:
    def __init__(self) -> None:
        self._jobs: Dict[str, Job] = {}
        self._lock = asyncio.Lock()

    async def add_job(self, job: Job) -> None:
        async with self._lock:
            self._jobs[job.id] = job

    async def list_jobs(self) -> List[Job]:
        async with self._lock:
            return list(self._jobs.values())

    async def get_job(self, job_id: str) -> Optional[Job]:
        async with self._lock:
            return self._jobs.get(job_id)

    async def update_job(self, job: Job) -> None:
        async with self._lock:
            self._jobs[job.id] = job


class GpuScheduler:
    def __init__(self, root_dir: Path) -> None:
        self.layout = DirLayout(root_dir)
        self.layout.setup_dirs()
        self.monitor = GpuMonitor()
        self.runner = ProcessRunner(self.layout.out)
        self.jobs = InMemoryJobStore()
        self.usage_tracker = UserUsageTracker() # Fair-shareìš©
        self.config = SlurmConfig()
        
        self._stop_event = asyncio.Event()
        self._poll_interval = 1.0

    async def submit_job(
        self,
        src_script: Path,
        user_id: str,
        vram_required: int = 2 * 1024**3,
        partition: str = "normal",
        qos: str = "standard",
    ) -> Job:
        job_id = str(uuid.uuid4())
        # to_run ë””ë ‰í† ë¦¬ë¡œ ì´ë™
        dst = self.layout.safe_rename(
            src_script, self.layout.to_run, new_name=f"{job_id}.py"
        )
        
        job = Job(
            id=job_id,
            script_path=dst,
            user_id=user_id,
            vram_required=vram_required,
            partition=partition,
            qos=qos,
            created_at=time.time(),
        )
        await self.jobs.add_job(job)
        return job

    async def list_jobs(self) -> List[Job]:
        return await self.jobs.list_jobs()

    async def get_job(self, job_id: str) -> Optional[Job]:
        return await self.jobs.get_job(job_id)

    # --------------------------------------------------------------------------
    # SLURM Priority Logic
    # --------------------------------------------------------------------------
    async def _calculate_slurm_priority(self, job: Job) -> float:
        """
        SLURM Multi-factor Priority Calculation
        Priority = Age + Fair-share + JobSize + Partition + QOS
        """
        # 1. Age Factor: ëŒ€ê¸° ì‹œê°„ (ìµœëŒ€ 1ì£¼ì¼ ëŒ€ê¸° ê¸°ì¤€ 1.0)
        max_age_sec = 7 * 24 * 3600
        age_factor = min(job.time_waiting / max_age_sec, 1.0)

        # 2. Fair-share Factor: ì‚¬ìš©ëŸ‰ì´ ë§Žì„ìˆ˜ë¡ 0ì— ìˆ˜ë ´
        # F = 1 / (1 + (UserUsage / DecayNorm))
        user_usage = await self.usage_tracker.get_usage(job.user_id)
        fs_factor = 1.0 / (1.0 + (user_usage / self.config.fairshare_decay_norm))
        
        # 3. Job Size Factor: VRAM ìš”êµ¬ëŸ‰ ê¸°ì¤€ (í° ìž‘ì—… ì„ í˜¸ ì‹œ)
        # ì˜ˆ: 80GBê°€ 1.0ì´ ë˜ë„ë¡ ì •ê·œí™”
        max_vram_ref = 80 * 1024**3
        size_factor = min(job.vram_required / max_vram_ref, 1.0)

        # 4. Partition Factor: ì„¤ì •ëœ íŒŒí‹°ì…˜ ì ìˆ˜
        part_factor = self.config.partitions.get(job.partition, 0.5)

        # 5. QOS Factor: ì„¤ì •ëœ QOS ì ìˆ˜
        qos_factor = self.config.qos_levels.get(job.qos, 0.5)

        # ìµœì¢… ì ìˆ˜ ê³„ì‚°
        prio = (
            (self.config.weight_age * age_factor) +
            (self.config.weight_fairshare * fs_factor) +
            (self.config.weight_job_size * size_factor) +
            (self.config.weight_partition * part_factor) +
            (self.config.weight_qos * qos_factor)
        )

        # ë””ë²„ê¹…/ë¡œê¹…ì„ ìœ„í•´ íŒ©í„° ì €ìž¥
        job._debug_factors = {
            "Age": age_factor,
            "FairShare": fs_factor,
            "Size": size_factor,
            "Partition": part_factor,
            "QOS": qos_factor,
            "RawUsage": user_usage
        }
        
        return prio

    async def _pick_next_job(self) -> Optional[Job]:
        """ìš°ì„ ìˆœìœ„ê°€ ê°€ìž¥ ë†’ì€ ìž‘ì—… ì„ íƒ"""
        jobs = await self.jobs.list_jobs()
        candidates: List[Job] = [
            j for j in jobs if j.status == "QUEUED"
        ]
        if not candidates:
            return None

        # ëª¨ë“  í›„ë³´ ìž‘ì—…ì˜ ìš°ì„ ìˆœìœ„ ìž¬ê³„ì‚° (ì‹œê°„ ê²½ê³¼, ì‚¬ìš©ëŸ‰ ë³€í™” ë°˜ì˜)
        for j in candidates:
            j.priority_score = await self._calculate_slurm_priority(j)

        # ì ìˆ˜ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
        candidates.sort(key=lambda j: j.priority_score, reverse=True)
        return candidates[0]

# ê¸°ì¡´ _find_available_gpu í•¨ìˆ˜ë¥¼ ì´ê±¸ë¡œ êµì²´í•˜ì„¸ìš”.
    async def _find_available_gpu(self, vram_required: int) -> Optional[int]:
        metrics = self.monitor.list_gpus()
        
        # [ìˆ˜ì •] í˜„ìž¬ ì‹¤í–‰ ì¤‘ì¸ ìž‘ì—…ì´ ìžˆëŠ”ì§€ í™•ì¸
        jobs = await self.jobs.list_jobs()
        running_jobs = [j for j in jobs if j.status == "RUNNING"]
        
        for m in metrics:
            if not m.is_healthy:
                continue

            # [ìˆ˜ì •] ê°€ìƒ GPU(virtual-gpu-0) íŠ¹ìˆ˜ ì²˜ë¦¬
            # ì‹¤ì œ GPUëŠ” ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ë³´ê³  íŒë‹¨í•˜ì§€ë§Œ, 
            # ê°€ìƒ GPUëŠ” ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ í•­ìƒ 0ì´ë¯€ë¡œ, ì‹¤í–‰ ì¤‘ì¸ ìž‘ì—… ê°œìˆ˜ë¡œ íŒë‹¨í•´ì•¼ í•¨.
            if m.name == "virtual-gpu-0" and len(running_jobs) > 0:
                # ì´ë¯¸ ëˆ„êµ°ê°€ ëŒê³  ìžˆë‹¤ë©´ ë°”ìœ ê²ƒìœ¼ë¡œ ê°„ì£¼í•˜ê³  ìŠ¤í‚µ
                continue

            # (ì‹¤ì œ GPUìš© ë¡œì§) ë©”ëª¨ë¦¬ ìž”ì—¬ëŸ‰ ì²´í¬
            free_mem = m.memory_total - m.memory_used
            if free_mem >= vram_required and m.memory_used < 1 * 1024**3:
                return m.gpu_id
                
        return None

    async def _launch_job(self, job: Job) -> None:
        gpu_id = await self._find_available_gpu(job.vram_required)

        if gpu_id is None:
            return

        # ìƒíƒœ ë³€ê²½
        start_time = time.time()
        new_script_path = self.layout.safe_rename(
            job.script_path, self.layout.running
        )
        job.script_path = new_script_path
        job.status = "RUNNING"
        job.assigned_gpu = gpu_id
        
        proc = self.runner.start_process(new_script_path, job.id, gpu_id)
        job.pid = proc.pid
        await self.jobs.update_job(job)

        # ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ë¡œ ì¢…ë£Œ ëŒ€ê¸° ë° í›„ì²˜ë¦¬
        asyncio.create_task(self._wait_and_finalize(job, proc, start_time))

    async def _wait_and_finalize(self, job: Job, proc, start_time: float) -> None:
        """í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ëŒ€ê¸° ë° Fair-share ì—…ë°ì´íŠ¸"""
        return_code = await asyncio.get_event_loop().run_in_executor(
            None, proc.wait
        )
        
        duration = time.time() - start_time
        
        # Fair-share: ì‚¬ìš©ëŸ‰ ì—…ë°ì´íŠ¸ (ì„±ê³µ/ì‹¤íŒ¨ ì—¬ë¶€ ìƒê´€ì—†ì´ ì ìœ  ì‹œê°„ë§Œí¼ ë¶€ê³¼)
        await self.usage_tracker.add_usage(job.user_id, duration)

        if return_code == 0:
            self.layout.safe_rename(job.script_path, self.layout.complete)
            job.status = "COMPLETED"
        else:
            self.layout.safe_rename(job.script_path, self.layout.fail)
            job.status = "FAILED"
            
        await self.jobs.update_job(job)

    async def run_forever(self) -> None:
        print("ðŸ”„ [SCHEDULER] ìŠ¤ì¼€ì¤„ëŸ¬ ë£¨í”„ ì‹œìž‘")
        try:
            while not self._stop_event.is_set():
                next_job = await self._pick_next_job()

                if next_job is not None:
                    await self._launch_job(next_job)
                    # ìž‘ì—…ì´ ì—†ìœ¼ë©´ ì¡°ê¸ˆ ë” ê¸¸ê²Œ ëŒ€ê¸°
                await asyncio.sleep(self._poll_interval)
        finally:
            self.monitor.shutdown()

    async def stop(self) -> None:
        self._stop_event.set()